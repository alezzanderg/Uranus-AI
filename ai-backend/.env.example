# AI Backend Configuration

# Server Configuration
DEBUG=true
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=INFO

# CORS Configuration
BACKEND_CORS_ORIGINS=["http://localhost:3000", "vscode-file://vscode-app", "http://localhost:8080"]

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_MODEL=gpt-4
OPENAI_MAX_TOKENS=2048

# Anthropic Claude Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Google Gemini Configuration
GOOGLE_API_KEY=your_google_api_key_here
GOOGLE_MODEL=gemini-pro

# xAI Grok Configuration
XAI_API_KEY=your_xai_api_key_here
XAI_MODEL=grok-beta

# DeepSeek Configuration
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_MODEL=deepseek-coder

# Ollama Configuration (for local models)
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama2

# Mistral Configuration
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_MODEL=mistral-large-latest

# Cohere Configuration
COHERE_API_KEY=your_cohere_api_key_here
COHERE_MODEL=command-r-plus

# Azure OpenAI Configuration (optional)
AZURE_OPENAI_API_KEY=your_azure_openai_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4

# Multi-Model Configuration
DEFAULT_MODEL=gpt-4
FALLBACK_MODELS=["gpt-3.5-turbo", "claude-3-haiku", "gemini-pro"]
ENABLE_MODEL_SWITCHING=true
ENABLE_COST_OPTIMIZATION=false
ENABLE_AUTO_FALLBACK=true

# Model Selection Preferences
PREFER_COST_EFFECTIVE=false
PREFER_SPEED=false
PREFER_QUALITY=true

# Rate Limiting
RATE_LIMIT_PER_MINUTE=60
RATE_LIMIT_PER_HOUR=1000

# Caching
ENABLE_RESPONSE_CACHING=true
CACHE_TTL_SECONDS=3600

# Monitoring and Analytics
ENABLE_USAGE_TRACKING=true
ENABLE_PERFORMANCE_MONITORING=true
ENABLE_ERROR_REPORTING=true

# Security
JWT_SECRET_KEY=your_jwt_secret_key_here
ENABLE_API_KEY_AUTH=false
API_KEY=your_api_key_here

# Database (optional, for storing usage stats)
DATABASE_URL=sqlite:///./uranus_ai.db

# Redis (optional, for caching and rate limiting)
REDIS_URL=redis://localhost:6379/0

# Webhook Configuration (optional)
WEBHOOK_URL=https://your-webhook-endpoint.com/webhook
WEBHOOK_SECRET=your_webhook_secret_here

# Feature Flags
ENABLE_STREAMING=true
ENABLE_FUNCTION_CALLING=true
ENABLE_VISION_MODELS=true
ENABLE_CODE_EXECUTION=false
ENABLE_WEB_SEARCH=false

# Model-specific Settings
# OpenAI
OPENAI_TEMPERATURE=0.7
OPENAI_TOP_P=1.0
OPENAI_FREQUENCY_PENALTY=0.0
OPENAI_PRESENCE_PENALTY=0.0

# Claude
CLAUDE_TEMPERATURE=0.7
CLAUDE_TOP_P=1.0
CLAUDE_TOP_K=40

# Gemini
GEMINI_TEMPERATURE=0.7
GEMINI_TOP_P=1.0
GEMINI_TOP_K=40

# Performance Tuning
MAX_CONCURRENT_REQUESTS=10
REQUEST_TIMEOUT_SECONDS=60
RETRY_ATTEMPTS=3
RETRY_DELAY_SECONDS=1

# Logging
LOG_FORMAT=json
LOG_FILE=logs/uranus-ai.log
LOG_ROTATION=daily
LOG_RETENTION_DAYS=30

